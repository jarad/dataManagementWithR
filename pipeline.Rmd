# Data Pipeline

## Introduction

Researchers often consider their data static, but in reality their data are 
dynamic and undergo modification from data collection to data analysis to 
data publication. 
It is helpful to have an understanding of this process so that data can be 
managed as the dynamic object. 

### A water analogy

An analogy for data is water. 
Water initially falls from the sky as rain, snow, sleet, or hail. 
From this point water can be absorbed into the ground, it can re-evaporate,
and it can join together to create brooks. 
These brooks join together to form creeks then branches and then into a river. 
This can be thought of as the process of data collection.

As the water flows downstream it has many purposes. 
For example, humans pull water from the stream for personal uses such as 
drinking, showering, cleaning, etc.
We also pull water for industrial purposes. 
Animals will also drink from the water. 
This is analogous to the many different purposes that data have including
for journal articles, for posters or oral presentations at a conference,
for stakeholder summaries, etc. 

The process described here is a natural process and natural systems go awry. 
A stream might flood or become dry. 
Water might get absorbed into the ground rather than flowing downstream for its
intended purpose. 
The water itself gets mixed all together so that it is unclear where it came
from. 
In dealing with our data, we do not want to allow these natural "disasters" to 
occur, but instead we want to exhibit more control over our data. 

### Data Pipeline

Rather than letting our data flow natural, we want to exhibit more control by
constructing a data pipeline. 
We want to be intentional about how we are dealing with the data.
We want to back the data up so that we have multiple identical rivers so that
a dried up riverbed does not end out scientific career. 

This data pipeline is constructed of a collection of individual steps that 
should be enumerated and described. 
These steps may include, but are not limited to, the following possible 
individual steps that we will describe in the remainder of this chapter:

- Data Collection
- Data Digitization
- Data Validation
- Data Transformation
- Data Backup
- Data Archiving
- Data Analysis
- Data Sharing
- Data Publication

These steps in a data pipeline are provided in a rough chronological order for
how they exist in the pipeline.
But it is important to note that some of these activities will occur 
simultaneously and therefore the pipeline must nimble enough to handle this 
simultaneous use. 


## Data Collection

The process of collecting data can be broadly categorized by how the data are 
obtained and what downstream processing will need to occur. 
These categories are 

- field data sheets
- electronic entry
- automatic recording

### Field data sheets

The least technologically-savvy approach is recording data on a field data sheet
or laboratory notebook. 
This means that someone is physically writing the data down onto a piece of 
loose-leaf or bound paper.
These data will need to be entered in the Data Digitization step in order for 
useful analyses to be performed. 

### Electronic entry

More and more field and laboratory data are entered directly into an electronic 
form by a researcher bypassing the physical piece of paper. 
This step bypasses the need for a Data Digitization step, but care is required
in developing the electronic entry system to ensure data quality because there
is no alternative record as a backup. 


### Automatic recording

With all automation, field and laboratory research is becoming automated via 
the use of electronic systems that automatically record data. 
These systems require calibration to ensure data quality and must be checked
frequently to ensure the systems are operating properly. 


## Data Digitization

When data are recorded in a field data sheet or a laboratory notebook, 
they will then need to be digitized, i.e. entered into an electronic form. 
This process needs to be documented so that folder structure and filenames as 
well as file structure are all consistent amongst the various times data are 
entered and individuals who are entering the data. 


## Data Validation

For all types of data collection, the data need to be checked and quality of the
data assessed. The process falls under the misleadingly exact heading of 
"validation". In reality, data are never validated, but instead we would like to
decrease the probability of data errors especially those types of data errors
that would drastically alter our understanding of the phenomenon under study. 

Within validation, we have two steps: data quality assurance and data quality 
control. Data quality assurance is a proactive process where we define how we 
will test the data. Data quality control is process of running those tests are 
fixing errors as they arise. The depth of testing directly controls the 
probability of undetected errors with more depth resulting in fewer undetected
errors. 


## Data Transformation

Typically digitized data are not in a format suitable for analysis, but instead
are typically found in many different files with multiple different structures.
Data transformation is the process of reading all these files in and organizing
the data in a way suitable for analysis.
There typically is not just one format, but different formats for different
types of analyses. 
The data transformation process includes joining data sets, unit conversions,
creating derived variables, etc. 

## Data Analysis

Once suitably transformed, the data are analyzed to extract information and 
provide scientific insights and, possibly, affect decisions. 
These analyses may be exploratory via figures and tables, but may also 
involve construction of statistical models which may also be summarized in 
figures and tables. 
The use of scripts in the data transformation and analysis steps will enable 
quick re-analysis when erroneous data are identified and corrected. 

## Data Backup

Unforeseen natural events as well as electronics failures suggest that we should
back up our data as well as the process used to transform and analyze that data. 
Thus we should have a plan for regular backups of our data and 
scripts so that in the event of destruction of some or all of our data and 
scripts, we can be back up and running as quickly as possible. 

## Data Archiving

Eventually we will move out of actively collecting, transforming, and analyzing
data. 
At this point, we will want to archive the data for future use. 
This can be a natural extension of backing up the data, but it can also 
including moving the data to an archiving platform where there is a guarantee
of future longevity of the data that may not exist with our current data storage
and backup. 


## Data Sharing

Typically data will be shared at many points along the data pipeline including
for interim analyses and analyses associated with a publication. 
Compared to the massive river of data being collected and transformed, 
data sharing will often be for only a subset of the overall collection.
Documenting who has access to what data and what data are shared with whom will 
ensure that when errors are identified, in any aspect of the data pipeline,
downstream users can be notified of those changes and can update their analyses.

## Data Publication

While data sharing is typically private, i.e. the data are shared with a select
number of individuals who are expected to keep the data in confidence, 
data publication is releasing the data (and scripts) publicly for the benefit 
of society. 
This benefit is increasingly understood by funding agencies who often require
publication of data in return for funding. 

In addition to the release of a location for data access, data publication 
usually requires effort spent to make the data useful by including documentation
of how the data were collected, steps taken for data validation, well-commented 
scripts associated with transformation and analysis, etc. 
Time spent during the development of the data pipeline to document the 
process reduces time spent on this documentation upon data publication.

## Summary
